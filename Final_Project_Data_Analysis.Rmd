---
title: "Final Project Data Analysis"
author: "Kristina Costa"
date: "12/1/2019"
output: pdf_document
toc: true
toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r echo = FALSE}
#requiring libraries for my data collection and manipulation first

require(tidyverse)
require(httr)
require(lubridate)
require(jsonlite)
require(tidycensus) #A package to help with Census API pulls! 
#here() starts at /Users/kristinacosta/Documents/Georgetown/
#PPOL_670_Data_Science/Costa-Final-Project
require(here)
here("HazardMitigationAssistanceProjects.csv", "DisasterDeclarationsSummaries.csv")

```

### Tidying my Pre Disaster Mitigation Program data

```{r}
#Starting with my Hazard Mitigation Grant Program project level data, since Grant Outcome-County is going to be my unit of analysis

dat_HMGP <- read.csv("HazardMitigationAssistanceProjects.csv")

#Filtering out only Pre Disaster Mitigation Program activities
#and assigning to a new object 
dat_PDM <- filter(dat_HMGP, programArea == "PDM")

#I only have PDMP data from FY 2000-2018, which will help me cut down my disaster declarations
dat_PDM <- dat_PDM %>% 
  arrange(., programFy) %>%  #arranging by fiscal year
  select(stateNumberCode, state, countyCode, county, projectType, 
         projectTitle, projectCounties, projectAmount, programFy) #keeping only variables I need 

#Checking for missing values 
dat_PDM %>% 
  select(everything()) %>% 
  summarise_all(funs(sum(is.na(.))))

#I'm missing 1004 county codes, presumably because grants were given statewide. 7 state codes are also missing, 
#and somehow so are 113 program FYs. If none of those overlap, then as many as 1124 of my 3789 observations are 
#missing key information for my later analysis. 

#FEMA is incredibly inconsistent in how it codes this data set. I'm going to drop entries 
#for which the county FIPs code is missing for the time being, as well as entries for which
#the county FIPs code is entered as a 0 because the grant was statewide. 

dat_PDM <- dat_PDM %>% 
  filter(countyCode > 0 & !is.na(countyCode)) %>% 
  filter(!is.na(stateNumberCode)) %>% #Filtering out those 7 missing state FIPs codes too
  filter(!is.na(programFy)) #I am also missing some Fiscal Year data, so removing those

#I now have 2,642 observations.

#My FEMA data sets don't have a single nice GEOID; the FIPS codes
#(where they exist) are in different columns and don't have leading zeroes. So I am going to 
#Start by trying to add leading zeroes to the disaster declaration data set, 
#then will mutate a new GEOID column 

dat_PDM <- dat_PDM %>% 
  mutate(stateNumberCode = sprintf("%02d", stateNumberCode)) %>% 
  mutate(countyCode = sprintf("%03d", countyCode)) %>% 
  unite(GEOID, stateNumberCode, countyCode, sep="") #Uniting my mutated fipsCodes to one GEOID

#I'm also going to separate the projectType code and projectType description into two columns
#Just in case there is human error in how any of the descriptions are written

dat_PDM <- dat_PDM %>% 
  separate('projectType', into = c("projectType", "projectDescription"), sep = ":")

#In 169 instances, FEMA coded at least 2 different projectType codes and descriptions into a single cell. 
#My code above dropped them since R was only expecting 2 values and one : separator. 
#Losing those data points takes a small amount of variation out of my data set, but it only affects .06 
#percent of all observations so hopefully won't affect my analysis too much. 

#I also need to add an indicator that the PDM counties are the "Yes" answers to what will ultimately be a categorization problem. 

dat_PDM <- dat_PDM %>% 
  add_column("Outcome" = "Grant")

#Need to winnow down my variables again now that I have a better grasp on the data
dat_PDM <- dat_PDM %>% 
  select(GEOID, Outcome, projectType, projectDescription, projectAmount, programFy)

```

### Tidying my Disaster Declarations data 

```{r}
#Reading in disaster declaration summary data

dat_FEMA <- read.csv("DisasterDeclarationsSummaries.csv")

dat_FEMA <- dat_FEMA %>% 
  filter(fyDeclared >= 2000) %>% #Filtering out only data since FY 2000 
  arrange(fyDeclared) %>% #Arranging by year of disaster
  filter(declarationType == 'DR') %>% #Filtering out only major disaster declarations
  select(state, declarationType, fyDeclared, incidentType,
         declarationTitle, fipsStateCode, fipsCountyCode,
         designatedArea) #keeping only variables I need for now

#Mutating the state and county FIPs codes into a uniform GEOID 

dat_FEMA <- dat_FEMA %>% 
  mutate(fipsStateCode = sprintf("%02d", fipsStateCode)) %>% 
  mutate(fipsCountyCode = sprintf("%03d", fipsCountyCode)) %>% 
  unite(GEOID, fipsStateCode, fipsCountyCode, sep="") #Uniting my mutated fipsCodes to one GEOID

#reordering my columns, dropping now-unnecessary variables 
dat_FEMA <- dat_FEMA %>% 
  select(GEOID, state, designatedArea, fyDeclared, incidentType, declarationTitle, declarationType)

#I care about the unit of analysis County-FEMA grant. That is, I want to analyze the characteristics of counties that have received FEMA grants. In my Disaster Declarations set, I have multiple instances for many GEOIDs, where they have had disaster declarations in multiple years for different disasters. While it would be perfectly interesting to analyze on the basis of disaster type, it would create a layer of complexity I'm just not ready for. So I'm going to count the instances of disaster declarations by GEOID, transmute to a new column, and then remove duplicate GEOIDs. 

dat_FEMA_simple <- dat_FEMA %>% #creating a new object for my simplified data so I don't lose variation in case I change my mind later
  add_count(GEOID, name="DisasterCount") %>% 
  distinct(GEOID, DisasterCount, .keep_all = TRUE) %>% 
  group_by(GEOID) %>% 
  filter(row_number() == 1) %>% 
  group_by(GEOID) %>% 
  slice(1) %>%  #grouping by GEOID and taking first instance from each group
  ungroup() %>% 
  select(GEOID, DisasterCount, state) #Selecting just my simplified variables and state abbreviation, since the first declaration year, disaster type, etc in each GEOID group isn't meaningful 

#There are 3,180 county-level observations in my simplified FEMA dataset.

```

### Acquiring and tidying my ACS data

```{r}

#Using the tidycensus package to pull ACS 5 year estimates for poverty rate 
#Note: API slides avail here https://www.census.gov/content/dam/Census/programs-surveys/acs/guidance/training-presentations/20180614_API.pdf


ACS_pov_dat <- get_acs(geography = "county", variables = "DP03_0119PE", key = "4b641703e32072356c733cd79067e522979bc17d")

#adjusting my ACS poverty data 
ACS_pov_dat <- ACS_pov_dat %>% 

select(GEOID, NAME, POVERTY_RATE = estimate) #dropping columns for Census variable name and margin of error, renaming 'estimate' column to reflect more useful variable name

#Using the tidycensus package to pull ACS 5 year estimates for racial demographics as county percentages
#I'm getting an error when I try to pull more than one variable, so will pull them individually
#and then join the tables together once I tidy them up a bit

ACS_white_dat <- get_acs(geography = "county", variables = "DP05_0037PE", key = "4b641703e32072356c733cd79067e522979bc17d")
ACS_black_dat <- get_acs(geography = "county", variables = "DP05_0038PE", key = "4b641703e32072356c733cd79067e522979bc17d")
ACS_native_dat <- get_acs(geography = "county", variables = "DP05_0039PE", key = "4b641703e32072356c733cd79067e522979bc17d")
ACS_asian_dat <- get_acs(geography = "county", variables = "DP05_0044PE", key = "4b641703e32072356c733cd79067e522979bc17d")
ACS_twoplus_dat <- get_acs(geography = "county", variables = "DP05_0058PE", key = "4b641703e32072356c733cd79067e522979bc17d")
ACS_hispanic_dat <- get_acs(geography = "county", variables = "DP05_0071PE", key = "4b641703e32072356c733cd79067e522979bc17d")

#Adjusting my ACS racial data; in all cases, dropping columns for Census variable name and margin of error, renaming 'estimate' column to reflect more useful variable name
ACS_white_dat <- ACS_white_dat %>% 
  select(GEOID, NAME, PCT_WHITE = estimate)
ACS_black_dat <- ACS_black_dat %>% 
  select(GEOID, NAME, PCT_BLACK = estimate)
ACS_native_dat <- ACS_native_dat %>% 
  select(GEOID, NAME, PCT_NATIVE = estimate)
ACS_asian_dat <- ACS_asian_dat %>% 
  select(GEOID, NAME, PCT_ASIAN = estimate)
ACS_twoplus_dat <- ACS_twoplus_dat %>% 
  select(GEOID, NAME, PCT_TWO_OR_MORE = estimate)
ACS_hispanic_dat <- ACS_hispanic_dat %>% 
  select(GEOID, NAME, PCT_HISPANIC = estimate)

#Now I want to join all my ACS data into one ACS data frame

ACS_all_dat <- list(ACS_pov_dat, ACS_white_dat, ACS_black_dat, ACS_native_dat, ACS_asian_dat, ACS_twoplus_dat, ACS_hispanic_dat) %>%   
  reduce(left_join, by = c("GEOID", "NAME"))

#There are 3,220 county-level observations in my ACS data set

```

```{r}
#Okay, I need to think about how to join my data sets together. 
#My unit of analysis is ultimately going to be Grant Outcome-County. It basically becomes a classification problem: Does the county get a grant or not? So I need to preserve ALL GEOIDs - there are 3,220 in the ACS County data (There are 3,142 counties and county equivalents in the 50 states and DC, and the ACS data also includes the 78 sub-territorial governmental units in Puerto Rico as county equivalents). 
#Some of my Pre Disaster Mitigation Grant recieving counties have received multiple grants in different years. I don't just want to collapse them in a count the way I did for the Disaster Declaration data, because I would ultimately like to be able to show how much money has flowed to different geographies when I do some of my fancier plots. 
#For the time being, I will full_join my three data sets together by GEOID. I will then be able to add "NO_GRANT" where there are NAs for Outcome, so I will have two categories at last. 

dat_PDM_FEMA_simple <- full_join(dat_PDM, dat_FEMA_simple, by="GEOID")
#This join gives me 4,678 observations. As a reminder there were 3,180 observations in the disaster declaration data - so since 2000, nearly every county in the US has declared a disaster at least once. This higher observation total indicates to me that there are quite a few Pre Disaster Mitigation counties that have received more than one grant. Also, FEMA uses FIPS codes in some places that aren't part of the ACS data set (like tribal areas). 

dat_all <- full_join(dat_PDM_FEMA_simple, ACS_all_dat, by="GEOID")

#This join somehow gives me 4,737 observations

```

```{r}
#Before I split into training and test, I need to fill in my other Outcome variable for all non-PDM recipients. 

dat_all <- dat_all %>% 
  mutate_at(vars(Outcome), ~replace_na(., "No_Grant")) #Replacing the NAs with No_Grant as the other categorical outcome

```

```{r}
#Saving my full data set for submission purposes 
write_csv(dat_all, "dat_all.csv")
```


### Training and Test Data Set 

```{r}
#Requiring additional libraries for my machine learning portion
require(caret)
require(recipes)
require(ranger)
require(rpart)

#Reordering my variables for my new data set for ease of use, then splitting into training and test data now that it's FINALLY TIDY

dat_all <- dat_all %>% select(Outcome, GEOID, State = state, Name = NAME, 
                   ProjectType = projectType, ProjectDescription = projectDescription, ProjectAmount = projectAmount,
                   PDMProgramFY = programFy, DisasterCount, POVERTY_RATE, PCT_WHITE, PCT_BLACK, PCT_NATIVE, PCT_ASIAN, 
                   PCT_TWO_OR_MORE, PCT_HISPANIC)

#breaking into training and test data
set.seed(1986) #setting a seed for replicability
inTraining <- createDataPartition(dat_all$Outcome,
                                  p = .75,
                                  list = FALSE) #partioning my data 75 / 25

training <- dat_all[inTraining,]
testing <- dat_all[-inTraining,]

#I now have 3,554 observations in my training dataset and 1,183 observations in my test dataset. 

```

### Exploratory statistics and visualizations

```{r, echo = FALSE}
#loading packages related to data visualization
require(ggplot2)
require(ggthemes)
require(scales)
require(sf) #for geographic maps
require(tmap) #for geographic maps 
require(rColorBrewer) #for nice color palettes 

#Top level glance at the statistical distributions of my combined training data 
summary(training)

#Looking at underlying relationships in training data 

sigma <- training %>% 
  select_if(is.numeric) %>% 
  na.omit(.) %>% 
  cor(.)

ggcorrplot::ggcorrplot(sigma, hc.order = TRUE, outline.color = "white", tl.cex = 5)

#Seeing if there are meaningful clusters of numeric variables

D <- training %>% 
  select_if(is.numeric) %>% 
  na.omit(.) 
clusters = hclust(dist(D), method = "complete")

pca_D <- FactoMineR::PCA(D, graph = F, scale.unit = T)
factoextra::fviz_eig(pca_D, addlabels = T,)

```

```{r}
#understanding the distribution of the financial amounts of Pre Disaster Mitigation projects awarded

PDM_amountplot <- training %>% ggplot(., aes(ProjectAmount)) +
  geom_histogram(fill = "steelblue", color = "white", bins = 100) +
  labs(x = "Value of Grants Awarded",
       y = "Number of Grants Awarded",
       fill = "",
       title = "Pre-Disaster Mitigation Grants Awarded",
       subtitle = "FY 2000-2018") +
  scale_x_continuous(limits = c(0, 1000000)) +
  ggthemes::theme_tufte()

#I have a huge amount of variation in the value of PDM grants awarded, from 0 (for technical assistance only)
#to $35 million. The higher awards are such outliers, though, that representing them on the chart below
#makes it basically useless, so limiting the visualization to $1 million to better illustrate the strong
#right skew 
PDM_amountplot
```


```{r}
#Saving the grant amount plot as a .png for my presentation
ggsave("PDM_amountplot.png", width = 7, height = 5)
```


```{r}
#Comparing poverty rate distribution in counties that received grants
#Vs counties that did not receive grants 

Povertyrateplot <- training %>% 
  ggplot(aes(x = POVERTY_RATE)) +
  geom_histogram(aes(color = Outcome, fill = Outcome),
                 alpha = 0.5, bins = 50) +
  theme(legend.position = "bottom") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  labs(x = "Poverty rate (%)",
       y = "Number of Instances (County-Grant)",
       title = "Poverty rate among Pre-Disaster Mitigation grant and non-grant counties")

  
Povertyrateplot


```


```{r}
#Saving the plot as a .png for my presentation
ggsave("Povertyrateplot.png", width = 7, height = 5)
```

```{r}
#Comparing number of counties by state that received
#Vs counties that did not receive grants 

Statedistroplot <- training %>% 
  ggplot(aes(x = State)) +
  geom_histogram(aes(color = Outcome, fill = Outcome),
                 alpha = 0.5, stat = "count") +
  theme(axis.text.x = element_text(angle = 90)) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  theme(legend.position = "bottom") +
    labs(x = "",
       y = "Number of Instances: Grant / No Grant",
       title = "Distribution of Grant / No-Grant County-Instances by State") 
  
  
Statedistroplot
```

```{r}
#Saving my grants by state distro as a .png for my presentation 
ggsave("Statedistroplot.png", width = 7, height = 5)
```

### Machine learning 

```{r}
#Looking at my outcome distributions 
training %>% 
  select(Outcome) %>% 
  gather(var, val) %>% 
  ggplot(aes(val)) + 
  geom_bar() +
  scale_y_log10() +
  facet_wrap(~var, scales = "free", ncol = 3) + 
  coord_flip()

#I have a good balance on the outcome 

```

```{r}
#Looking at my racial distributions 
training %>% 
  select(PCT_WHITE, PCT_BLACK, PCT_NATIVE, PCT_ASIAN, PCT_TWO_OR_MORE, PCT_HISPANIC) %>% 
  gather(var, val) %>% 
  ggplot(aes(val)) + 
  geom_histogram(bins = 40) +
  scale_y_log10() +
  facet_wrap(~var, scales = "free", ncol = 3) + 
  coord_flip()

#I have a small number of missing values I'll need to impute. There also appear to be some ranges of racial representation that aren't present in the data. 
```

```{r}
summary(training)
```

```{r}
#I have NAs in the data which need to be treated differently... I want to impute missing ACS derived data, but for disaster count, NA should be 0. There isn't really a way to impute the missing states, so I'm inclined to not use state as a predictor (which makes some sense since I care about the characteristics of counties anyway). I'm obviously also missing PDMProgramFY data for any county that doesn't have a grant, so that could create a perfect prediction problem -- so I will also drop the FY from my predictor set. 
#For this initial set of models, I don't care about the amount of the grant -- so going to drop that variable, and the project descriptions too

training_mod <- training %>% 
  select(Outcome, GEOID, DisasterCount, POVERTY_RATE, PCT_WHITE, PCT_BLACK, PCT_NATIVE, PCT_ASIAN, PCT_TWO_OR_MORE, PCT_HISPANIC) %>% 
  mutate_at(., vars(DisasterCount), funs(ifelse(is.na(.), 0, .)))

#Applying changes to my testing data set too 

testing_mod <- testing %>% 
  select(Outcome, GEOID, DisasterCount, POVERTY_RATE, PCT_WHITE, PCT_BLACK, PCT_NATIVE, PCT_ASIAN, PCT_TWO_OR_MORE, PCT_HISPANIC) %>% 
  mutate_at(., vars(DisasterCount), funs(ifelse(is.na(.), 0, .)))

#Setting up my recipe such that the outcome is grant outcome 
rcp <- 
  recipe(Outcome ~ ., training_mod) %>% 
  step_medianimpute(all_numeric()) %>%  #imputing any missing values for ACS-derived data 
  step_range(all_numeric()) %>% #normalizing my scale. 
  prep()

#applying the recipe to both my training and testing data sets 
training_mod2 <- bake(rcp, training_mod)
testing_mod2 <- bake(rcp, testing_mod)

#Setting up cross-validation

set.seed(1986) #setting my seed for replication
folds <- createFolds(training_mod2$Outcome, k = 5)
sapply(folds, length)

#Setting up my control conditions 

control_conditions <- 
  trainControl(method = 'cv', #K-fold cross validation
               summaryFunction = twoClassSummary, #classification problem
               classProbs = TRUE, #classification problem
               index = folds) #indices for folds

```

```{r}
require(stargazer) #requiring stargazer to render a nice table for my report
stargazer(training_mod, out = "summarystats.html") #Outputting as HTML for ease of integrating into my RMD
```


```{r}
#Running a logistic regression model
#I don't want to use GEOID as a predictor, but also don't want to disassociate the GEOID from the data, 
#so selecting the other predictors only
mod_logit <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "glm", 
                   metric = "ROC",
                   trControl = control_conditions) 

mod_logit

```

```{r}
#Running a KNN model
#I don't want to use GEOID as a predictor, but also don't want to disassociate the GEOID from the data, 
#so selecting the other predictors only
mod_knn <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "knn", 
                   metric = "ROC",
                   trControl = control_conditions) 

mod_knn
```

```{r}
plot(mod_knn)
#The improvements are on a straight line so I haven't yet found my best performing KNN model... so I'm going to expand my tuning parameters 
```

```{r}
#Setting new tuning parameters
knn_tune <- expand.grid(k = c(1,3,10,20,30,40,50))

#running a second KNN model

mod_knn2 <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "knn", 
                   metric = "ROC",
                   tuneGrid = knn_tune, #adding the tuning parameters 
                   trControl = control_conditions) 

mod_knn2
```

```{r}
plot(mod_knn2)
#20 neighbors is my best performing model 
```

```{r}
#Running a CART model 

#I don't want to use GEOID as a predictor, but also don't want to disassociate the GEOID from the data, 
#so selecting the other predictors only
mod_cart <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "rpart", 
                   metric = "ROC",
                   trControl = control_conditions) 

mod_cart
```

```{r}
plot(mod_cart)
```

```{r}
rattle::fancyRpartPlot(mod_cart$finalModel)

#Interestingly the tree initially splits on the PCT_ASIAN variable. 

```

```{r}
#setting tuning parameters to let the tree grow deeper
tune_cart2 <- expand.grid(cp = c(0.0061347)) #complexity parameter

#Running a second CART model

mod_cart2 <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "rpart", 
                   metric = "ROC",
                   tuneGrid = tune_cart2, #tuning parameters
                   trControl = control_conditions) 

mod_cart2


```

```{r}
CART_deep_tree <- rattle::fancyRpartPlot(mod_cart2$finalModel)
```

```{r}
#Running a random forest model 

#I don't want to use GEOID as a predictor, but also don't want to disassociate the GEOID from the data, 
#so selecting the other predictors only
mod_rf <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "ranger", 
                   metric = "ROC",
                   trControl = control_conditions) 

mod_rf


```

```{r}
plot(mod_rf)
```

```{r}
#Trying out some support vector machine models too, just for fun. 
#Linear boundary 
#I don't want to use GEOID as a predictor, but also don't want to disassociate the GEOID from the data, 
#so selecting the other predictors only

library(kernlab)
mod_svm_linear <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "svmLinear", 
                   metric = "ROC",
                   tuneGrid = expand.grid(C = c(.5, 1)), #adding tuning parameters
                   trControl = control_conditions) 

mod_svm_linear


```

```{r}
#Polynomial boundary 
#I don't want to use GEOID as a predictor, but also don't want to disassociate the GEOID from the data, 
#so selecting the other predictors only

mod_svm_poly <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "svmPoly", 
                   metric = "ROC",
                   trControl = control_conditions) 

mod_svm_poly

```

```{r}
#Radial boundary
#I don't want to use GEOID as a predictor, but also don't want to disassociate the GEOID from the data, 
#so selecting the other predictors only

mod_svm_radial <- train(Outcome ~ DisasterCount + POVERTY_RATE + PCT_WHITE + 
                             PCT_BLACK + PCT_NATIVE + PCT_ASIAN + PCT_TWO_OR_MORE + PCT_HISPANIC,
                   data = training_mod2,
                   method = "svmRadial", 
                   metric = "ROC",
                   trControl = control_conditions) 

mod_svm_radial


```

### Model comparison

```{r}
#organizing model outcomes as a list

mod_list <- list(logit = mod_logit, 
                 knn1 = mod_knn, 
                 knn2 = mod_knn2, 
                 cart1 = mod_cart, 
                 cart2 = mod_cart2, 
                 rf = mod_rf, 
                 svm_linear = mod_svm_linear, 
                 svm_poly = mod_svm_poly, 
                 svm_radial = mod_svm_radial)

#Generating a plot to compare output

mod_outcomes <- dotplot(resamples(mod_list))
mod_outcomes
```


### Examining predictive power of variables in RF model

```{r}
require(vip) #requiring VIP package 
```

```{r}
pred_wrapper <- function(object, newdata){
  #creating wrapper function so vip can calculate a prediction
  predict(object, data = newdata, type = "response")$predictions[,"Grant"]
}

#Creating a variable importance plot
permute_imp_plot <- 
  vip(mod_rf$finalModel,
      data = training_mod2,
      target = training_mod2$Outcome,
      train = training_mod2 %>% select(-Outcome, -GEOID), 
      #I need to not include GEOID, as well as Outcome, because it is perfectly predictive
      #(each GEOID can only belong to one county/county-equivalent, by definition)
      reference_class = "Grant",
      method = "permute",
      pred_wrapper = pred_wrapper,
      fill = "steelblue") +
  theme_light()
  
  
permute_imp_plot
```

```{r}
#Saving my variable importance plot

ggsave("variable_importance.png", width = 7, height = 5)

```


### Testing data on RF model 

```{r}
#looking at the predictive performance of the RF model 

pred <- predict(mod_rf, newdata = testing_mod2)
confusionMatrix(table(pred, testing_mod2$Outcome))

```

###Machine learning 2: Differences by grant amount

```{r}
#I'm curious if there's any difference between the characteristics of counties receiving small grants (<$75,000) and those receiving larger grants (up to $50 million). 

#Of course, in some instances counties have received multiple grants... so I think I'm going to sum a total grant amount by county before doing this analysis.

#Starting once again with my full data set

dat_all_split <- dat_all %>% 
  group_by(GEOID, Outcome, DisasterCount, POVERTY_RATE, PCT_WHITE, PCT_BLACK, PCT_NATIVE, PCT_ASIAN, PCT_TWO_OR_MORE, PCT_HISPANIC) %>% 
  summarise_at(vars(ProjectAmount), funs(sum)) %>% #Summarizing the ProjectAmount variable for each county
  ungroup()
```

```{r}
#I have a different outcome this time... I want to know whether a county is going to sort into the high grant category (>$100,000) or the low grant category (<$100,000). So I need to mutate a new outcome column. 

dat_all_split <- dat_all_split %>% 
  mutate(grant_level = ifelse(ProjectAmount > 75000, "high", "low"))

```

```{r}
#I only care about counties that received grants, so I can filter out only those counties

dat_all_split <- dat_all_split %>% 
  filter(Outcome == "Grant")

```

```{r}
#Reordering and renaming my columns

dat_all_split <- dat_all_split %>% 
  select(GEOID, GrantLevel = grant_level, ProjectAmount, DisasterCount, POVERTY_RATE, PCT_WHITE, PCT_BLACK, PCT_NATIVE, PCT_ASIAN, PCT_TWO_OR_MORE, PCT_HISPANIC)

```

```{r}
#breaking into training and test data
set.seed(1986) #setting a seed for replicability
inTrainingGrant <- createDataPartition(dat_all_split$GrantLevel,
                                  p = .75,
                                  list = FALSE) #partioning my data 75 / 25

trainingGrant <- dat_all_split[inTrainingGrant,]
testingGrant <- dat_all_split[-inTrainingGrant,]


```

```{r}
summary(trainingGrant)
#I still have missing values even only looking at successful grants, so going to need to impute those again using the same methods as previously 
```

```{r}
#Changing disaster count NAs to 0s in the training data 
trainingGrant <- trainingGrant %>% 
  #I'm also dropping the projectAmount variable at this point to avoid perfect prediction
  select(GrantLevel, DisasterCount, POVERTY_RATE, PCT_WHITE, PCT_BLACK, PCT_NATIVE, PCT_ASIAN, PCT_TWO_OR_MORE, PCT_HISPANIC) %>% 
  mutate_at(., vars(DisasterCount), funs(ifelse(is.na(.), 0, .)))
```

```{r}
#Changing disaster count NAs to 0s in the testing data
testingGrant <- testingGrant %>% 
  select(GrantLevel, DisasterCount, POVERTY_RATE, PCT_WHITE, PCT_BLACK, PCT_NATIVE, PCT_ASIAN, PCT_TWO_OR_MORE, PCT_HISPANIC) %>% 
  mutate_at(., vars(DisasterCount), funs(ifelse(is.na(.), 0, .)))
```

```{r}
#Setting up my recipe such that the outcome is grant level
rcpGrant <- 
  recipe(GrantLevel ~ ., trainingGrant) %>% 
  step_medianimpute(all_numeric()) %>%  #imputing any missing values for ACS-derived data 
  step_range(all_numeric()) %>% #normalizing my scale. 
  prep()

```

```{r}
#applying the recipe to both my training and testing data sets 
trainingGrant_mod <- bake(rcpGrant, trainingGrant)
testingGrant_mod <- bake(rcpGrant, testingGrant)

```

```{r}
#Setting up cross-validation

set.seed(1986) #setting my seed for replication
foldsGrant <- createFolds(trainingGrant_mod$GrantLevel, k = 5)
sapply(foldsGrant, length)
```

```{r}
#Setting up my control conditions 

control_conditions_Grant <- 
  trainControl(method = 'cv', #K-fold cross validation
               summaryFunction = twoClassSummary, #classification problem
               classProbs = TRUE, #classification problem
               index = foldsGrant) #indices for folds

```

```{r}
#I'm just running a Random Forest this time because I want to see what the most influential variables are when it comes to grant amount 

#I dropped GEOID and ProjectAmount as predictors, so can use all my remaining variables
mod_rf_grant <- train(GrantLevel ~ .,
                   data = trainingGrant_mod,
                   method = "ranger", 
                   metric = "ROC",
                   trControl = control_conditions_Grant) 

mod_rf_grant
```

```{r}
pred_wrapper_high <- function(object, newdata){
  #creating wrapper function so vip can calculate a prediction
  predict(object, data = newdata, type = "response")$predictions[,"high"]
}


```

```{r}
#Creating a variable importance plot
permute_imp_plot_high <- 
  vip(mod_rf_grant$finalModel,
      data = trainingGrant_mod,
      target = trainingGrant_mod$GrantLevel,
      train = trainingGrant_mod %>% select(-GrantLevel),
      reference_class = "high",
      method = "permute",
      pred_wrapper = pred_wrapper_high)

permute_imp_plot_high
```

```{r}
pred <- predict(mod_rf_grant, newdata = testingGrant_mod)
confusionMatrix(table(pred, testingGrant_mod$GrantLevel))
```

```{r}
#The ROC for this second round of machine learning is pretty pathetic, so I'm not going to use these results in my report. 
```


### My Albatross: Mapping 

```{r}
#Let's try to make a map of poverty rates in counties where grants have been awarded 

#I only care about where grants have been awarded, so filtering out my No_Grant data and everything but GEOID

PDM_map_dat <- training %>% 
  filter(., Outcome == "Grant") %>% 
  select(GEOID, State) %>% 
  filter(!is.na(State)) %>% 
  filter(!str_detect(State, "AS")) %>%  #removing territories that don't appear in the rNaturalEarth US map
  filter(!str_detect(State, "GU")) %>% 
  filter(!str_detect(State, "TM")) %>% 
  filter(!str_detect(State, "MP")) %>% 
  filter(!str_detect(State, "VI")) %>% 
  filter(!str_detect(State, "PR")) %>% 
  distinct() #removing duplicate counties (have received more than 1 grant)

```

```{r}
#Getting spatial data from Census for poverty data set 

ACS_pov_map <- get_acs(geography = "county", variables = "DP03_0119PE", 
                       output = "tidy", geometry = TRUE,
                       shift_geo = TRUE, key = "4b641703e32072356c733cd79067e522979bc17d") %>% 
  select(GEOID, PovertyRate = estimate, geometry) #renaming and dropping unneeded variables 

```

```{r}
#Joining my ACS poverty with geography data to my PDM map data; I only want to keep 
#rows that match on GEOID so I am using an inner_join

PDM_map_dat2 <- inner_join(ACS_pov_map, PDM_map_dat, by = "GEOID")

#The issue arose because I was joining the ACS_pov_map object to the PDM_map_dat object, which 
#did not contain geometric data. I needed to join them the other way around in order to produce
#a df with sf features instead of just a tibble (with a geometry column... but no overall geometric
#properties, evidently. H/t to Nathan for figuring THAT one out.)
```

```{r}
#Calling shapefile from rnaturalearth package

require(rnaturalearth)
require(rnaturalearthdata)
require(rnaturalearthhires)

states_shp <- ne_states(returnclass = "sf",
                        country = "United States of America") %>% 
  filter(name != "Alaska" & name != "Hawaii") #dropping AK and HI for initial map

#Set up continental US projection

us.proj <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

```

```{r}
# Create a state FIPS field & Aggregate the county layer by state 
states <- mutate(ACS_pov_map, STFIPS = stringr::str_sub(GEOID, 1, 2)) %>% 
  tmaptools::aggregate_map(by = "STFIPS")

#Making the map for PDM counties with fixed width bins 

poverty_map <- tm_shape(PDM_map_dat2,
                     projection = us.proj) +
  tm_fill("PovertyRate",
          palette = "PuBuGn",
          title = "Poverty Rate") + 
  tm_shape(states,
           projection = us.proj) +
  tm_borders() +
  tm_layout(main.title = "Fig. 5: Poverty Rate in PDM-Grant Counties",
            legend.title.size = .8,
            legend.text.size = 0.5,
            legend.position = c("LEFT", "BOTTOM"),
            main.title.size = .8,
            frame = F) +
  tm_credits("Source: 2013-2017 5-year American Community Survey and FEMA Hazard Mitigation Grant Program | 
             Counties receiving at least one Pre-Disaster Mitigation Grant since 2000",
             position = c("RIGHT", "BOTTOM"),
             size = .3)

#Making the map for all counties with fixed width bins, too

poverty_map_all <- tm_shape(ACS_pov_map,
                     projection = us.proj) +
  tm_fill("PovertyRate",
          palette = "PuBuGn",
          title = "Poverty Rate") + 
  tm_shape(states,
           projection = us.proj) +
  tm_borders() +
  tm_layout(main.title = "Fig. 6: Poverty Rate in All U.S. Counties",
            legend.title.size = .8,
            legend.text.size = 0.5,
            legend.position = c("LEFT", "BOTTOM"),
            main.title.size = .8,
            frame = F) +
  tm_credits("Source: 2013-2017 5-year American Community Survey",
             position = c("RIGHT", "BOTTOM"),
             size = .3)

```

```{r}
#Saving the map

tmap_save(poverty_map, "poverty_rate_PDM_counties_map.png", width = 1920, height = 1080, asp = 0)
tmap_save(poverty_map_all, "poverty_rate_all_counties.png", width = 1920, height = 1080, asp = 0)

```

```{r}
#Given the results of my models, I want to make a few more maps: One showing the concentration of Asian households in PDM grant counties vs. the US as a whole (since that was such a strong determinant in my rf model); and maps showing the disaster count by counties receiving PDM grants vs. nationwide. 

#Building my Asian maps

ACS_asian_map <- get_acs(geography = "county", variables = "DP05_0044PE", output = "tidy", geometry = TRUE, shift_geo = TRUE, key = "4b641703e32072356c733cd79067e522979bc17d") %>% 
    select(GEOID, PCT_ASIAN = estimate, geometry)

#Joining my Asian ACS data to my PDM map data; I only want to keep 
#rows that match on GEOID so I am using an inner_join

ACS_asian_map2 <- inner_join(ACS_asian_map, PDM_map_dat, by = "GEOID")
```

```{r}
#Creating my map of % Asian individuals by grant-receiving county
#I can still use the state FIPS and aggregated county work-around to get outlines for HI and AK from above

#Making the map with jenks  bins 

Grant_asian_map <- tm_shape(ACS_asian_map2,
                     projection = us.proj) +
  tm_fill("PCT_ASIAN",
          palette = "PuBuGn",
          title = "Percent Asian",
          style = "jenks") + 
  tm_shape(states,
           projection = us.proj) +
  tm_borders() +
  tm_layout(main.title = "Fig. 3: Percent of population identifying as Asian in PDM-grant counties",
            legend.title.size = .8,
            legend.text.size = 0.5,
            legend.position = c("LEFT", "BOTTOM"),
            main.title.size = .8,
            frame = F) +
  tm_credits("Source: 2013-2017 5-year American Community Survey and FEMA Hazard Mitigation Grant Program | 
             Counties receiving at least one Pre-Disaster Mitigation Grant since 2000 | Jenks breaks",
             position = c("RIGHT", "BOTTOM"),
             size = .3)

#Saving the map
tmap_save(Grant_asian_map, "pct_asian_PDM_counties.png", width = 1920, height = 1080, asp = 0)
```

```{r}
#Creating my map of % Asian individuals in all counties
#I can still use the state FIPS and aggregated county work-around to get outlines for HI and AK from above

#Making the map with jenks bins 

All_asian_map <- tm_shape(ACS_asian_map,
                     projection = us.proj) +
  tm_fill("PCT_ASIAN",
          palette = "PuBuGn",
          title = "Percent Asian",
          style = "jenks") + #want to use jenks breaks so data self-sorts
  tm_shape(states,
           projection = us.proj) +
  tm_borders() +
  tm_layout(main.title = "Fig. 4: Percent of population identifying as Asian in all US counties",
            legend.title.size = .8,
            legend.text.size = 0.5,
            legend.position = c("LEFT", "BOTTOM"),
            main.title.size = .8,
            frame = F) +
  tm_credits("Source: 2013-2017 5-year American Community Survey and FEMA Hazard Mitigation Grant Program | 
            Jenks breaks",
             position = c("RIGHT", "BOTTOM"),
             size = .3)

#Saving the map
tmap_save(All_asian_map, "pct_asian_all_counties.png", width = 1920, height = 1080, asp = 0)

```

```{r}
#Mapping PDM counties by disaster count 

PDM_map_disasters <- training %>% 
  filter(., Outcome == "Grant") %>% 
  select(GEOID, State, DisasterCount) %>% 
  filter(!is.na(State)) %>% #removing instances where a state isn't specified
  filter(!str_detect(State, "AS")) %>%  #removing territories that don't appear in the rNaturalEarth US map
  filter(!str_detect(State, "GU")) %>% 
  filter(!str_detect(State, "TM")) %>% 
  filter(!str_detect(State, "MP")) %>% 
  filter(!str_detect(State, "VI")) %>% 
  filter(!str_detect(State, "PR")) %>% 
  distinct() #removing duplicate counties (have received more than 1 grant)

```

```{r}
#Joining to a previous Census pull to get geometry data by GEOID 
PDM_map_disasters2 <- inner_join(ACS_asian_map, PDM_map_disasters, by = "GEOID") %>% 
  select(GEOID, DisasterCount, geometry)

```

```{r}
#Creating my map of disaster declarations in PDM counties 

#I can still use the state FIPS and aggregated county work-around to get outlines for HI and AK from above

#Making the map with jenks bins 

Disaster_count_map <- tm_shape(PDM_map_disasters2,
                     projection = us.proj) +
  tm_fill("DisasterCount",
          palette = "PuBuGn",
          title = "Number of Disasters",
          style = "jenks") + #I want jenks breaks so the data self-sorts 
  tm_shape(states,
           projection = us.proj) +
  tm_borders() +
  tm_layout(main.title = "Fig 7: Number of Disasters since 2000 in PDM-grant counties",
            legend.title.size = .8,
            legend.text.size = 0.5,
            legend.position = c("LEFT", "BOTTOM"),
            main.title.size = .8,
            frame = F) +
  tm_credits("Source: Federal Emergency Management Agency Disaster Declarations | 
             Counties receiving at least one Pre-Disaster Mitigation Grant since 2000",
             position = c("RIGHT", "BOTTOM"),
             size = .3)

#Saving the map
tmap_save(Disaster_count_map, "disaster_count_PDM_counties.png", width = 1920, height = 1080, asp = 0)
```


```{r}
#Mapping all counties by disaster declarations

All_map_disasters <- training %>% 
  select(GEOID, State, DisasterCount) %>% 
  filter(!is.na(State)) %>% #removing instances where a state isn't specified
  filter(!str_detect(State, "AS")) %>%  #removing territories that don't appear in the rNaturalEarth US map
  filter(!str_detect(State, "GU")) %>% 
  filter(!str_detect(State, "TM")) %>% 
  filter(!str_detect(State, "MP")) %>% 
  filter(!str_detect(State, "VI")) %>% 
  filter(!str_detect(State, "PR")) %>% 
  distinct() #removing duplicate counties (have received more than 1 grant)

#Joining to a previous Census pull to get geometry data by GEOID 
All_map_disasters2 <- inner_join(ACS_asian_map, All_map_disasters, by = "GEOID") %>% 
  select(GEOID, DisasterCount, geometry)

#Creating my map of disaster declarations in all counties 

#I can still use the state FIPS and aggregated county work-around to get outlines for HI and AK from above

#Making the map with jenks bins 

All_disaster_count_map <- tm_shape(All_map_disasters2,
                     projection = us.proj) +
  tm_fill("DisasterCount",
          palette = "PuBuGn",
          title = "Number of Disasters",
          style = "jenks") + #I want jenks breaks so the data self-sorts 
  tm_shape(states,
           projection = us.proj) +
  tm_borders() +
  tm_layout(main.title = "Fig. 8: Number of Disasters since 2000 in all US counties",
            legend.title.size = .8,
            legend.text.size = 0.5,
            legend.position = c("LEFT", "BOTTOM"),
            main.title.size = .8,
            frame = F) +
  tm_credits("Source: Federal Emergency Management Agency Disaster Declarations",
             position = c("RIGHT", "BOTTOM"),
             size = .3)

#Saving the map
tmap_save(All_disaster_count_map, "disaster_count_all_counties.png", width = 1920, height = 1080, asp = 0)

```

